# -*- coding: utf-8 -*-
"""CLIP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M3K2o7SCohxPwF69wJP7bUfosesLIR0G
"""

!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

!pip install ftfy regex tqdm
!pip install git+https://github.com/openai/CLIP.git

import torch
import clip

device = "cuda" if torch.cuda.is_available() else "cpu"
CLIPmodel, CLIPpreprocess = clip.load("ViT-B/32", device=device)

print("CLIP and dependencies installed successfully!")

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d adityajn105/flickr8k

!pip install kaggle
from google.colab import files
files.upload()  # Upload your kaggle.json file

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Example: Download Flickr8k dataset
!kaggle datasets download -d adityajn105/flickr8k
!unzip flickr8k.zip -d /content/flickr8k
captions_file = "/content/flickr8k/captions.txt"
image_dir = "/content/flickr8k/Images"

import os
from collections import defaultdict
import torch
import pickle
from sklearn.metrics.pairwise import cosine_similarity

# Function to parse the captions file and extract captions for the first 10 unique images
def load_test_data(captions_file, image_dir, num_images=10):
    image_captions = defaultdict(list)

    with open(captions_file, 'r') as file:
        for line in file:
            image_name, caption = line.strip().split(',', 1)
            image_path = os.path.join(image_dir, image_name)
            if os.path.exists(image_path) and len(image_captions) < num_images:
                if len(image_captions[image_path]) < 5:
                    image_captions[image_path].append(caption)

    test_data = [(img_path, captions) for img_path, captions in image_captions.items()]
    return test_data

# Function to load saved image features
def load_saved_image_features(file_path="image_features.pkl"):
    with open(file_path, 'rb') as f:
        image_features_dict = pickle.load(f)
    print(f"Loaded {len(image_features_dict)} image features from {file_path}")
    return image_features_dict

# Function to evaluate accuracy with 5 captions per image
def evaluate_accuracy_with_captions(test_data, image_features_dict, model, device, top_k=1):
    correct_retrievals = 0
    total_caption_samples = 0

    with torch.no_grad():
        for true_image_path, captions in test_data:
            for caption in captions:
                caption_features = model.encode_text(clip.tokenize([caption]).to(device)).detach().cpu()
                similarities = {path: torch.nn.functional.cosine_similarity(caption_features, feat).item()
                                for path, feat in image_features_dict.items()}
                top_k_paths = sorted(similarities, key=similarities.get, reverse=True)[:top_k]
                if true_image_path in top_k_paths:
                    correct_retrievals += 1
                total_caption_samples += 1

    accuracy = correct_retrievals / total_caption_samples
    return accuracy

# Load the first 10 unique images with up to 5 captions each
captions_file = "/content/flickr8k/captions.txt"
image_dir = "/content/flickr8k/Images"
test_data = load_test_data(captions_file, image_dir, num_images=10)

# Path to the saved features file
saved_features_file = "image_features.pkl"

# Load the saved image features
image_features_dict = load_saved_image_features(saved_features_file)

# Evaluate accuracy with the first 10 images and their captions
top_k = 5
accuracy = evaluate_accuracy_with_captions(test_data, image_features_dict, CLIPmodel, device, top_k)
print(f"Top-{top_k} Accuracy: {accuracy:.2%}")

import matplotlib.pyplot as plt
from PIL import Image

# Function to visualize top-K retrieved images for each caption
def visualize_results(test_data, image_features_dict, model, device, top_k=5):
    with torch.no_grad():
        for true_image_path, captions in test_data:
            print(f"True Image: {true_image_path}")
            plt.figure(figsize=(15, 5))

            # Display the true image
            true_image = Image.open(true_image_path)
            plt.subplot(1, top_k + 1, 1)
            plt.imshow(true_image)
            plt.axis('off')
            plt.title("True Image")

            for idx, caption in enumerate(captions):
                # Encode the caption
                caption_features = model.encode_text(clip.tokenize([caption]).to(device)).detach().cpu()

                # Calculate cosine similarity with precomputed image features
                similarities = {path: torch.nn.functional.cosine_similarity(caption_features, feat).item()
                                for path, feat in image_features_dict.items()}

                # Get the top K matches
                top_k_paths = sorted(similarities, key=similarities.get, reverse=True)[:top_k]

                print(f"Caption: {caption}")
                print(f"Top-{top_k} Retrieved Images:")

                # Visualize the top-K images
                for i, retrieved_path in enumerate(top_k_paths):
                    retrieved_image = Image.open(retrieved_path)
                    plt.subplot(1, top_k + 1, i + 2)
                    plt.imshow(retrieved_image)
                    plt.axis('off')
                    plt.title("Correct" if retrieved_path == true_image_path else "Retrieved")

                plt.show()

visualize_results(test_data, image_features_dict, model, device, top_k=10)

!pip install torchviz

from torchviz import make_dot

# Create dummy inputs
dummy_image = torch.randn(1, 3, 224, 224).to(device)
dummy_text = clip.tokenize(["This is a dummy caption"]).to(device)

# Forward pass
image_features = model.encode_image(dummy_image)
text_features = model.encode_text(dummy_text)

# Visualize the computation graph
make_dot((image_features, text_features), params=dict(model.named_parameters()))

# Freeze all layers
for name, param in model.named_parameters():
    param.requires_grad = False

# Unfreeze the last transformer block and text projection layer
for name, param in model.named_parameters():
    if "transformer.resblocks.11" in name or "text_projection" in name:
        param.requires_grad = True

# Verify trainable layers
trainable_layers = [name for name, param in model.named_parameters() if param.requires_grad]
print(f"Trainable Layers: {trainable_layers}")

from torch.utils.data import DataLoader

# Dataset class
class FineTuneDataset(torch.utils.data.Dataset):
    def __init__(self, image_features_dict, captions_file):
        self.image_features_dict = image_features_dict
        self.captions = []

        with open(captions_file, 'r') as file:
            for line in file:
                image_name, caption = line.strip().split(',', 1)
                image_path = next((key for key in image_features_dict if key.endswith(image_name)), None)
                if image_path:
                    self.captions.append((image_path, caption))

    def __len__(self):
        return len(self.captions)

    def __getitem__(self, idx):
        image_path, caption = self.captions[idx]
        image_feature = self.image_features_dict[image_path]
        return image_feature.squeeze(0), caption

# Load dataset
captions_file = "/content/flickr8k/captions.txt"
dataset = FineTuneDataset(image_features_dict, captions_file)

# DataLoader
batch_size = 8  # Keep batch size small to save memory
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

import torch.nn.functional as F

# Contrastive loss
def contrastive_loss(image_features, text_features, temperature=0.1):
    logits = (image_features @ text_features.T) / temperature
    labels = torch.arange(len(image_features)).to(image_features.device)
    loss = F.cross_entropy(logits, labels)
    return loss

# Optimizer for fine-tuned layers
optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)

from tqdm import tqdm
import os

# Define checkpoint directory and initialize start_epoch
save_dir = "./checkpoints"
os.makedirs(save_dir, exist_ok=True)
checkpoint_path = os.path.join(save_dir, "checkpoint_last.pth")

# Check for an existing checkpoint
if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch']  # Start from the next epoch
    print(f"Resuming training from epoch {start_epoch}")
else:
    start_epoch = 0
    print("No checkpoint found, starting training from epoch 0")

# Training loop
for epoch in range(start_epoch, 3):  # Resume from start_epoch if necessary
    model.train()
    total_loss = 0

    # Use tqdm to wrap the DataLoader
    with tqdm(dataloader, desc=f"Epoch {epoch + 1}", unit="batch") as tepoch:
        for image_features, captions in tepoch:
            # Move data to device
            image_features = image_features.to(device)
            text_tokens = clip.tokenize(captions).to(device)

            # Forward pass for text encoder
            text_features = model.encode_text(text_tokens)

            # Normalize features
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)

            # Compute loss
            loss = contrastive_loss(image_features, text_features)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            # Update the progress bar with the current loss
            tepoch.set_postfix(loss=loss.item())

    # Save epoch progress to a file
    with open(os.path.join(save_dir, "training_progress.txt"), "a") as f:
        f.write(f"Epoch {epoch + 1}, Average Loss: {total_loss / len(dataloader):.4f}\n")

    # Save checkpoint for the last epoch
    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss / len(dataloader),
    }, checkpoint_path)
    print(f"Checkpoint saved for epoch {epoch + 1}")

final_model_path = "./checkpoints/fine_tuned_clip_final.pth"
torch.save(model.state_dict(), final_model_path)
print(f"Final model saved at {final_model_path}")

# Evaluate the trained model
accuracy = evaluate_accuracy_with_captions(test_data, image_features_dict, model, device, top_k=50)
print(f"Final Top-5 Accuracy: {accuracy:.2%}")

import matplotlib.pyplot as plt

# Load training progress
losses = []
with open("/content/accuracy_progress.txt", "r") as f:
    for line in f:
        if "Accuracy" in line:
            loss = float(line.strip().split(": ")[1])
            losses.append(loss)

# Plot the loss
plt.plot(range(1, len(losses) + 1), losses, marker='o')
plt.title("Accuracy Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.grid()
plt.show()

import matplotlib.pyplot as plt

# Load training progress
losses = []
with open("./checkpoints/training_progress.txt", "r") as f:
    for line in f:
        if "Average Loss" in line:
            loss = float(line.strip().split(": ")[1])
            losses.append(loss)

# Plot the loss
plt.plot(range(1, len(losses) + 1), losses, marker='o')
plt.title("Training Loss Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid()
plt.show()

def test_caption(model, image_features_dict, caption, device, top_k=10):
    model.eval()
    with torch.no_grad():
        # Tokenize and encode the caption
        text_features = model.encode_text(clip.tokenize([caption]).to(device))
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)

        # Compute similarities
        similarities = {path: torch.nn.functional.cosine_similarity(text_features, feat).item()
                        for path, feat in image_features_dict.items()}

        # Retrieve top-K images
        top_k_paths = sorted(similarities, key=similarities.get, reverse=True)[:top_k]
        return top_k_paths

# Example caption
example_caption = "A man walking in street with an umbrella"
top_images = test_caption(model, image_features_dict, example_caption, device, top_k=10)

print("Top matching images for the caption:")
for image_path in top_images:
    print(image_path)

# Visualize retrieved images
for image_path in top_images:
    image = Image.open(image_path)
    plt.imshow(image)
    plt.title(f"Retrieved Image:")
    plt.axis("off")
    plt.show()

def test_caption(model, image_features_dict, caption, device, top_k=10):
    model.eval()
    with torch.no_grad():
        # Tokenize and encode the caption
        text_features = model.encode_text(clip.tokenize([caption]).to(device))
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)

        # Compute similarities
        similarities = {path: torch.nn.functional.cosine_similarity(text_features, feat).item()
                        for path, feat in image_features_dict.items()}

        # Retrieve top-K images
        top_k_paths = sorted(similarities, key=similarities.get, reverse=True)[:top_k]
        return top_k_paths

# Example caption
example_caption = "Guys playing"
top_images = test_caption(CLIPmodel, image_features_dict, example_caption, device, top_k=10)

print("Top matching images for the caption:")
for image_path in top_images:
    print(image_path)

# Visualize retrieved images
for image_path in top_images:
    image = Image.open(image_path)
    plt.imshow(image)
    plt.title(f"Retrieved Image:")
    plt.axis("off")
    plt.show()

import clip
import torch

# Load CLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Load the saved model weights
checkpoint_path = "./checkpoints/fine_tuned_clip_final.pth"  # Path to the saved model
model.load_state_dict(torch.load(checkpoint_path, map_location=device))
model.eval()  # Set the model to evaluation mode

print("Model loaded successfully and ready for inference!")